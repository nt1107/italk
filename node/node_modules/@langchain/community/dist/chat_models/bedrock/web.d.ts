import { EventStreamCodec } from "@smithy/eventstream-codec";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { type BaseChatModelParams, BaseChatModel, LangSmithParams, BaseChatModelCallOptions } from "@langchain/core/language_models/chat_models";
import { BaseLanguageModelInput, ToolDefinition } from "@langchain/core/language_models/base";
import { Runnable, RunnableToolLike } from "@langchain/core/runnables";
import { AIMessageChunk, BaseMessage, BaseMessageChunk } from "@langchain/core/messages";
import { ChatGenerationChunk, ChatResult } from "@langchain/core/outputs";
import { StructuredToolInterface } from "@langchain/core/tools";
import type { SerializedFields } from "../../load/map_keys.js";
import { BaseBedrockInput, type CredentialType } from "../../utils/bedrock/index.js";
type AnthropicTool = Record<string, unknown>;
export declare function convertMessagesToPromptAnthropic(messages: BaseMessage[], humanPrompt?: string, aiPrompt?: string): string;
/**
 * Function that converts an array of messages into a single string prompt
 * that can be used as input for a chat model. It delegates the conversion
 * logic to the appropriate provider-specific function.
 * @param messages Array of messages to be converted.
 * @param options Options to be used during the conversion.
 * @returns A string prompt that can be used as input for a chat model.
 */
export declare function convertMessagesToPrompt(messages: BaseMessage[], provider: string): string;
export interface BedrockChatCallOptions extends BaseChatModelCallOptions {
    tools?: (StructuredToolInterface | AnthropicTool | ToolDefinition | RunnableToolLike)[];
}
export interface BedrockChatFields extends Partial<BaseBedrockInput>, BaseChatModelParams {
}
/**
 * A type of Large Language Model (LLM) that interacts with the Bedrock
 * service. It extends the base `LLM` class and implements the
 * `BaseBedrockInput` interface. The class is designed to authenticate and
 * interact with the Bedrock service, which is a part of Amazon Web
 * Services (AWS). It uses AWS credentials for authentication and can be
 * configured with various parameters such as the model to use, the AWS
 * region, and the maximum number of tokens to generate.
 *
 * The `BedrockChat` class supports both synchronous and asynchronous interactions with the model,
 * allowing for streaming responses and handling new token callbacks. It can be configured with
 * optional parameters like temperature, stop sequences, and guardrail settings for enhanced control
 * over the generated responses.
 *
 * @example
 * ```typescript
 * import { BedrockChat } from 'path-to-your-bedrock-chat-module';
 * import { HumanMessage } from '@langchain/core/messages';
 *
 * async function run() {
 *   // Instantiate the BedrockChat model with the desired configuration
 *   const model = new BedrockChat({
 *     model: "anthropic.claude-v2",
 *     region: "us-east-1",
 *     credentials: {
 *       accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID!,
 *       secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY!,
 *     },
 *     maxTokens: 150,
 *     temperature: 0.7,
 *     stopSequences: ["\n", " Human:", " Assistant:"],
 *     streaming: false,
 *     trace: "ENABLED",
 *     guardrailIdentifier: "your-guardrail-id",
 *     guardrailVersion: "1.0",
 *     guardrailConfig: {
 *       tagSuffix: "example",
 *       streamProcessingMode: "SYNCHRONOUS",
 *     },
 *   });
 *
 *   // Prepare the message to be sent to the model
 *   const message = new HumanMessage("Tell me a joke");
 *
 *   // Invoke the model with the message
 *   const res = await model.invoke([message]);
 *
 *   // Output the response from the model
 *   console.log(res);
 * }
 *
 * run().catch(console.error);
 * ```
 *
 * For streaming responses, use the following example:
 * @example
 * ```typescript
 * import { BedrockChat } from 'path-to-your-bedrock-chat-module';
 * import { HumanMessage } from '@langchain/core/messages';
 *
 * async function runStreaming() {
 *   // Instantiate the BedrockChat model with the desired configuration
 *   const model = new BedrockChat({
 *     model: "anthropic.claude-3-sonnet-20240229-v1:0",
 *     region: "us-east-1",
 *     credentials: {
 *       accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID!,
 *       secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY!,
 *     },
 *     maxTokens: 150,
 *     temperature: 0.7,
 *     stopSequences: ["\n", " Human:", " Assistant:"],
 *     streaming: true,
 *     trace: "ENABLED",
 *     guardrailIdentifier: "your-guardrail-id",
 *     guardrailVersion: "1.0",
 *     guardrailConfig: {
 *       tagSuffix: "example",
 *       streamProcessingMode: "SYNCHRONOUS",
 *     },
 *   });
 *
 *   // Prepare the message to be sent to the model
 *   const message = new HumanMessage("Tell me a joke");
 *
 *   // Stream the response from the model
 *   const stream = await model.stream([message]);
 *   for await (const chunk of stream) {
 *     // Output each chunk of the response
 *     console.log(chunk);
 *   }
 * }
 *
 * runStreaming().catch(console.error);
 * ```
 */
export declare class BedrockChat extends BaseChatModel<BedrockChatCallOptions, AIMessageChunk> implements BaseBedrockInput {
    model: string;
    region: string;
    credentials: CredentialType;
    temperature?: number | undefined;
    maxTokens?: number | undefined;
    fetchFn: typeof fetch;
    endpointHost?: string;
    /** @deprecated Use as a call option using .bind() instead. */
    stopSequences?: string[];
    modelKwargs?: Record<string, unknown>;
    codec: EventStreamCodec;
    streaming: boolean;
    usesMessagesApi: boolean;
    lc_serializable: boolean;
    trace?: "ENABLED" | "DISABLED";
    guardrailIdentifier: string;
    guardrailVersion: string;
    guardrailConfig?: {
        tagSuffix: string;
        streamProcessingMode: "SYNCHRONOUS" | "ASYNCHRONOUS";
    };
    protected _anthropicTools?: AnthropicTool[];
    get lc_aliases(): Record<string, string>;
    get lc_secrets(): {
        [key: string]: string;
    } | undefined;
    get lc_attributes(): SerializedFields | undefined;
    _identifyingParams(): Record<string, string>;
    _llmType(): string;
    static lc_name(): string;
    constructor(fields?: BedrockChatFields);
    invocationParams(options?: this["ParsedCallOptions"]): {
        tools: AnthropicTool[];
        temperature: number | undefined;
        max_tokens: number | undefined;
        stop: string[] | undefined;
        modelKwargs: Record<string, unknown> | undefined;
        guardrailConfig: {
            tagSuffix: string;
            streamProcessingMode: "SYNCHRONOUS" | "ASYNCHRONOUS";
        } | undefined;
    };
    getLsParams(options: this["ParsedCallOptions"]): LangSmithParams;
    _generate(messages: BaseMessage[], options: Partial<this["ParsedCallOptions"]>, runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    _generateNonStreaming(messages: BaseMessage[], options: Partial<this["ParsedCallOptions"]>, _runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    _signedFetch(messages: BaseMessage[], options: this["ParsedCallOptions"], fields: {
        bedrockMethod: "invoke" | "invoke-with-response-stream";
        endpointHost: string;
        provider: string;
    }): Promise<Response>;
    _streamResponseChunks(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
    _readChunks(reader: any): {
        [Symbol.asyncIterator](): AsyncGenerator<Uint8Array, void, unknown>;
    };
    _combineLLMOutput(): {};
    bindTools(tools: (StructuredToolInterface | AnthropicTool | ToolDefinition | RunnableToolLike)[], _kwargs?: Partial<this["ParsedCallOptions"]>): Runnable<BaseLanguageModelInput, BaseMessageChunk, this["ParsedCallOptions"]>;
}
/**
 * @deprecated Use `BedrockChat` instead.
 */
export declare const ChatBedrock: typeof BedrockChat;
export {};
